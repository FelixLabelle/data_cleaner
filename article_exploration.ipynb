{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key for this data are:  ['ï»¿uri', 'url', 'date', 'time', 'source.uri', 'source.title', 'lang', 'title', 'body']\n",
      "There are 5 sources\n",
      "There are 100 articles, 100 of which are unique\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from collections import Counter\n",
    "\n",
    "data = []\n",
    "keys = []\n",
    "text = []\n",
    "with open('canadian_blockade_articles.csv', 'r') as csvfile:\n",
    "    data = [row for row in csv.DictReader(csvfile)]\n",
    "\n",
    "keys = [key for key in data[0]]\n",
    "print(\"Key for this data are: \", keys)\n",
    "\n",
    "# Extract data\n",
    "sentences = [item['body'] for item in data]\n",
    "titles = [item['title'] for item in data]\n",
    "sources = [item['source.title'] for item in data]\n",
    "\n",
    "# Calculate statistics\n",
    "src_counts = Counter(sources)\n",
    "unique_titles = Counter(titles)\n",
    "\n",
    "# Display statistics\n",
    "print(\"There are {} sources\".format(len(src_counts)))\n",
    "print(\"There are {} articles, {} of which are unique\".format(len(titles), len(unique_titles)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def sent_processor(sent, stopwords = {}):\n",
    "    return [word for word in gensim.utils.simple_preprocess(sent, deacc=True) if word not in stopwords]\n",
    "\n",
    "tokenized_titles = [sent_processor(title,stopwords) for title in titles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 86 articles filtered and 0 duplicates found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['that', 'this']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KEY_WORD_THRESHOLD = 2\n",
    "\n",
    "# This is a bit hardcore, it removes EVERY duplicate item\n",
    "# TODO: Keep the latest article if a duplicate is found\n",
    "def duplicateRemoval(data):\n",
    "    return [item for i, item in enumerate(data) if item not in data[:i] + data[i+1:]]\n",
    "\n",
    "def occurenceCount(title, filter_words):\n",
    "    return sum([filter_word in title for filter_word in filter_words])\n",
    "\n",
    "def flatten_list(lists):\n",
    "    return [item for nested_list in lists for item in nested_list]\n",
    "\n",
    "# TODO: Automatically create this word list\n",
    "key_words = set(['railway','rail','railroad','canada','trudeau','mohawk','protest','indigenous','pipeline','blockade'])\n",
    "\n",
    "# Filter articles\n",
    "scored_titles = [(data, occurenceCount(title,key_words)) for data, title in zip(data, tokenized_titles)]\n",
    "filtered_articles = [data for data,key_word_occ in scored_titles if KEY_WORD_THRESHOLD <= key_word_occ]\n",
    "filtered_len = len(filtered_articles)\n",
    "filtered_articles = duplicateRemoval(filtered_articles)\n",
    "\n",
    "# Report filtering results\n",
    "deduplicated_len = len(filtered_articles)\n",
    "articles_filtered = len(data) - filtered_len\n",
    "duplicate_articles = filtered_len - deduplicated_len\n",
    "print(\"There were {} articles filtered and {} duplicates found\".format(articles_filtered, duplicate_articles))\n",
    "#duplicateRemoval([\"the\",\"that\",\"this\",\"the\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "ROOT_PATH = \"data\"\n",
    "article_ids = []\n",
    "\n",
    "for article in filtered_articles:\n",
    "    article_id = article['ï»¿uri']\n",
    "    article_ids.append(article_id)\n",
    "    with open(os.path.join(ROOT_PATH, article_id + '.txt'),'w+',encoding='utf-8') as article_fh:\n",
    "        article_fh.writelines(article['body'])\n",
    "\n",
    "with open(os.path.join(ROOT_PATH,'filelist.txt'),'w+') as lst_fh:\n",
    "    lst_fh.writelines(['{}\\n'.format(article) for article in article_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1900 unique words\n"
     ]
    }
   ],
   "source": [
    "# Utils to sort\n",
    "\n",
    "\n",
    "# Create vocab\n",
    "from gensim.corpora import Dictionary\n",
    "dct = Dictionary(processed_text)\n",
    "corpus = [dct.doc2bow(line) for line in processed_text]\n",
    "\n",
    "print(\"There are {} unique words\".format(len(dct)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "from gensim.matutils import corpus2dense\n",
    "#TODO: Investigate using a sparse matrix (From a theoritical point of view)\n",
    "model = TfidfModel(corpus)\n",
    "tfidfs = [model[item] for item in corpus]\n",
    "tfidfs = corpus2dense(tfidfs,len(dct),len(tfidfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({6: 14, 2: 12, 9: 13, 3: 14, 0: 4, 1: 9, 8: 11, 5: 3, 4: 12, 7: 8})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=10)\n",
    "kmeans.fit_predict(tfidfs.T)\n",
    "Counter(kmeans.predict(tfidfs.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
