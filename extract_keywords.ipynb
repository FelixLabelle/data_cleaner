{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import unidecode\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'articles_society.json'\n",
    "with open(path) as infile:\n",
    "    articles = json.loads(infile.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_by_location = dict()\n",
    "for location in articles:\n",
    "    articles_by_location[location] = [text['body'] for source in articles[location] for text in articles[location][source]['articles']['results']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    # remove accents\n",
    "    text = unidecode.unidecode(text)\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_combined_by_location = dict()\n",
    "for location in articles_by_location:\n",
    "    articles_combined_by_location[location] = ' '.join(articles_by_location[location])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(strip_accents='unicode', stop_words='english')\n",
    "tfidf_vectors = vectorizer.fit_transform(list(articles_combined_by_location.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Venezuela', 'France', 'Hong Kong', 'Iran'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_combined_by_location.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_df_idf(index, vectorizer, tfidf_vectors):\n",
    "    df = pd.DataFrame(tfidf_vectors[index].T.todense(), index=vectorizer.get_feature_names(), columns=[\"tfidf\"])\n",
    "    return df.sort_values(by=[\"tfidf\"],ascending=False)\n",
    "\n",
    "# VENEZUELA\n",
    "venezuela_kw = extract_df_idf(0, vectorizer, tfidf_vectors)\n",
    "# France\n",
    "france_kw = extract_df_idf(1, vectorizer, tfidf_vectors)\n",
    "# HONG KONG\n",
    "hong_kong_kw = extract_df_idf(2, vectorizer, tfidf_vectors)\n",
    "# IRAN\n",
    "iran_kw = extract_df_idf(3, vectorizer, tfidf_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_kw(kw_lists):\n",
    "    kw_intersections = set()\n",
    "    for list1, list2 in itertools.combinations(kw_lists, 2):\n",
    "        kw_intersections = kw_intersections.union(set(list1).intersection(set(list2)))\n",
    "#     print(kw_intersections)\n",
    "    out_kw_lists = [list(kw_list) for kw_list in kw_lists]\n",
    "    for i in range(len(kw_lists)):\n",
    "        for elt in kw_lists[i]:\n",
    "            if elt in kw_intersections:\n",
    "#                 print('removing {} from {}'.format(elt, kw_list))\n",
    "                out_kw_lists[i].remove(elt)\n",
    "#                 print('kw_list {}'.format(kw_list))\n",
    "    return out_kw_lists\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_kw = 200\n",
    "cleaned_lists = clean_kw([\n",
    "    venezuela_kw.index[:num_kw],\n",
    "    france_kw.index[:num_kw],\n",
    "    hong_kong_kw.index[:num_kw],\n",
    "    iran_kw.index[:num_kw]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_kw_lists.txt', 'w') as out_file:\n",
    "    for kw_list in cleaned_lists:\n",
    "        out_file.write(', '.join(kw_list) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewed_kw = [['maduro', 'guaido', 'venezuela', 'caracas', 'venezuelan', 'juan', 'venezuelans', 'nicolas', 'lopez', 'chavez', 'cabello', 'leopoldo', 'diosdado', 'hyperinflation', 'latin', 'shortages', 'cucuta', 'hugo', 'padrino'],\n",
    "['paris', 'macron', 'yellow', 'french', 'france', 'champs', 'elysees', 'vest', 'vests', 'arc', 'triomphe', 'emmanuel', 'avenue', 'christophe', 'castaner', 'jackets', 'philippe', 'elysee', 'marseille', 'michel', 'edouard'],\n",
    "['hong', 'kong', 'china', 'lam', 'chinese', 'extradition', 'beijing', 'mainland', 'carrie', 'wong', 'long', 'yuen', 'cheung', 'chan', 'communist', 'xi', 'kowloon', 'xinjiang', 'kongers'],\n",
    "['iran', 'iranian', 'tehran', 'soleimani', 'khamenei', 'irgc', 'ali', 'ayatollah', 'fadavi', 'drone', 'iranians', 'irbil', 'erbil', 'rouhani', 'mousavi', 'khuzestan', 'persian', 'zarif']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
